from datetime import datetime, time

import numpy as np
import pandas as pd

phases = {
    "Night": [time(0, 0, 0), time(5, 59, 59)],
    "Morning": [time(6, 0, 0), time(11, 59, 59)],
    "Afternoon": [time(12, 0, 0), time(17, 59, 59)],
    "Evening": [time(18, 0, 0), time(23, 59, 59)]
}

class DataManager:
    def __init__(self, fichier):
        self.Data = self.charger_data(fichier)

    @staticmethod
    def charger_data(fichier):
        try:
            return pd.read_excel(fichier)
        except Exception as e:
            print(f"Erreur lors du chargement du fichier : {e}")
            return None

    @staticmethod
    def enregistrer_data(data, fichier):
        try:
            data.to_excel(fichier, index=False)
            print(f"Fichier enregistrÃ© : {fichier}")
        except Exception as e:
            print(f"Erreur lors de l'enregistrement : {e}")

    def get_data(self):
        return self.Data

    def set_data(self, fichier="../Data/Data1.xlsx"):
        self.Data = self.charger_data(fichier)
        if self.Data is not None:
            print("âœ… RÃ©cupÃ©ration des donnÃ©es rÃ©ussie")



    def Div_Data_Day(self, Day):
        print("On va diviser la data par rapport au jour")
        D = self.Data.copy()
        D["activation_date"] = pd.to_datetime(D["activation_date"],dayfirst=False,errors="coerce")
        mask = D["activation_date"].dt.date == Day.date()
        return D[mask]

    def diviser_par_tranches(self, nb_jours=22):
        df = self.Data.copy()
        df["activation_date"] = pd.to_datetime(df["activation_date"], dayfirst=False, errors="coerce")
        df = df.dropna(subset=["activation_date"])
        df = df.sort_values("activation_date")
        # Calcul de la date minimale et maximale
        min_date = df["activation_date"].min().normalize()
        max_date = df["activation_date"].max().normalize()
        # Calcule la diffÃ©rence en jours entre chaque 'activation_date' et la date minimale
        df["delta_jours"] = (df["activation_date"].dt.normalize() - min_date).dt.days
        # Calcule le numÃ©ro de tranche basÃ© sur 'delta_jours'
        df["tranche_jours"] = df["delta_jours"] // nb_jours + 1
        # Optionnel : formatage des tranches en libellÃ©, par exemple "J1 Ã  J4"
        df["tranche_label"] = df["tranche_jours"].apply(
            lambda i: f"J{(i - 1) * nb_jours + 1} Ã  J{i * nb_jours}"
        )
        # Suppression de la colonne 'delta_jours' qui n'est plus nÃ©cessaire
        df = df.drop(columns=["delta_jours"])

        return df

    @staticmethod
    def supprimer_doublons_msisdn(df):
        # VÃ©rifie si df est None
        if df is None:
            raise ValueError("Le DataFrame passÃ© en argument est None")

        # VÃ©rifie si la colonne 'msisdn' existe dans df
        if 'msisdn' in df.columns:
            df = df.drop_duplicates(subset=['msisdn'])
            return df
        else:
            raise ValueError("'msisdn' n'est pas une colonne dans le DataFrame")

    @staticmethod
    def join_strict_PDV(D1, D2):
        D1 = D1.copy()
        D2 = D2.copy()

        # ðŸ§¼ Nettoyage des dates
        D1["activation_date"] = pd.to_datetime(D1["activation_date"], errors='coerce')
        D2["date_debut_analyse"] = pd.to_datetime(D2["date_debut_analyse"], errors='coerce')
        D2["date_fin_analyse"] = pd.to_datetime(D2["date_fin_analyse"], errors='coerce')

        # ðŸ” Colonnes utiles uniquement dans D2
        D2 = D2[["pdv_id", "date_debut_analyse", "date_fin_analyse", "Cluster", "Anomalie", "Score_anomalie",
                 "Variable_responsable","Taux_impact"]]

        # â›“ï¸ CrÃ©ation dâ€™une table Ã©largie par produit cartÃ©sien sur pdv_id (pour tester les intervalles)
        merged = pd.merge(D1, D2, on="pdv_id", how="left")

        # ðŸ•’ Pour chaque ligne, vÃ©rifier si activation_date âˆˆ [date_debut_analyse, date_fin_analyse]
        mask = (merged["activation_date"] >= merged["date_debut_analyse"]) & \
               (merged["activation_date"] <= merged["date_fin_analyse"])

        # ðŸŽ¯ Conserver uniquement les correspondances valides (si multiples correspondances : plusieurs lignes)
        merged_valid = merged[mask].copy()

        # âœ… Si plusieurs correspondances, garder la premiÃ¨re
        merged_valid = merged_valid.drop_duplicates(subset=["pdv_id", "activation_date"])

        # ðŸš« Pour les lignes sans correspondance, on les retrouve via anti-jointure
        D1_keys = D1[["pdv_id", "activation_date"]].drop_duplicates()
        matched_keys = merged_valid[["pdv_id", "activation_date"]]
        unmatched = pd.merge(D1_keys, matched_keys, on=["pdv_id", "activation_date"], how="left", indicator=True)
        unmatched = unmatched[unmatched["_merge"] == "left_only"].drop(columns=["_merge"])

        # â›” Attribuer Cluster = -1 pour les enregistrements sans correspondance
        unmatched["Cluster"] = -1
        final = pd.concat([merged_valid[["pdv_id", "activation_date", "Cluster", "Anomalie", "Score_anomalie",
                                         "Variable_responsable","Taux_impact"]], unmatched], ignore_index=True)

        # ðŸ” Fusion finale avec la data source (D1)
        D1 = D1.merge(final, on=["pdv_id", "activation_date"], how="left")
        D1["Cluster"] = D1["Cluster"].fillna(-1).astype(int)

        # L'Anomalie, Score_anomalie et Variable_responsable sont dÃ©jÃ  ajoutÃ©es depuis D2, donc rien Ã  ajouter ici

        return D1

    @staticmethod
    def join_strict_Sub(D1, D2):
        D1 = D1.copy()
        D2 = D2.copy()

        # ðŸ”„ Conversion des dates
        D1["activation_date"] = pd.to_datetime(D1["activation_date"], errors='coerce')
        D2["date_debut_analyse"] = pd.to_datetime(D2["date_debut_analyse"], errors='coerce')
        D2["date_fin_analyse"] = pd.to_datetime(D2["date_fin_analyse"], errors='coerce')

        # ðŸ§ª Colonnes utiles uniquement dans D2
        D2 = D2[
            ["subscriber_id_number", "date_debut_analyse", "date_fin_analyse", "Cluster", "Anomalie", "Score_anomalie",
             "Variable_responsable","Taux_impact"]]

        # ðŸ”— Jointure large
        merged = pd.merge(D1, D2, on="subscriber_id_number", how="left")

        # ðŸ“Œ Marquer les correspondances valides
        mask = (merged["activation_date"] >= merged["date_debut_analyse"]) & \
               (merged["activation_date"] <= merged["date_fin_analyse"])
        merged_valid = merged[mask].copy()

        # ðŸ§  Pour chaque activation, on garde le premier Cluster valide (si plusieurs)
        merged_valid = merged_valid.drop_duplicates(subset=["subscriber_id_number", "activation_date"])

        # ðŸŽ¯ ClÃ©s de jointure
        keys = ["subscriber_id_number", "activation_date"]

        # ðŸ” Trouver les activations sans correspondance valide
        all_keys = D1[keys].drop_duplicates()
        matched_keys = merged_valid[keys]
        unmatched = pd.merge(all_keys, matched_keys, on=keys, how="left", indicator=True)
        unmatched = unmatched[unmatched["_merge"] == "left_only"].drop(columns=["_merge"])
        unmatched["Cluster"] = -1

        # ðŸ” Fusion des valides + non appariÃ©s
        final = pd.concat(
            [merged_valid[keys + ["Cluster", "Anomalie", "Score_anomalie", "Variable_responsable","Taux_impact"]], unmatched],
            ignore_index=True)

        # ðŸ’¡ Jointure finale avec la source
        D1 = D1.merge(final, on=keys, how="left")
        D1["Cluster"] = D1["Cluster"].fillna(-1).astype(int)

        # L'Anomalie, Score_anomalie et Variable_responsable sont dÃ©jÃ  ajoutÃ©es depuis D2, donc rien Ã  ajouter ici

        return D1

    @staticmethod
    def Div_Data_phase(data, phase):
        if data is None:
            print("âš ï¸ Erreur : les donnÃ©es fournies sont vides")
            return pd.DataFrame()

        print("On va diviser la data par rapport au moment de la journÃ©e")
        D = data.copy()
        D["activation_date"] = pd.to_datetime(D["activation_date"],dayfirst=True ,errors="coerce")
        if phase in phases:
            mask = D["activation_date"].dt.time.between(phases[phase][0], phases[phase][1])
            D_filtered = D[mask].copy()
            D_filtered["time_phase"] = phase
            return D_filtered
        else:
            print("âš ï¸ Phase invalide")
            return pd.DataFrame()

    def drop_col_no_pdv(self, data):
        col_names = ["activation_date", "pdv_id", "code_pdv_wilaya", "status"]
        data = data[col_names]
        return data
